{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP using RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9x3VrGGMPDN",
        "colab_type": "text"
      },
      "source": [
        "### Word Embeddings\n",
        "\n",
        "In word embeddings, each word is represented by an n dimension vector. The words with similar meaning, have their vectors in the same direction. The words with opposite meaning, have their vectors in opposite direction.\n",
        "\n",
        "In other words, vectors with similar meaning have a small angle between them and the vectors with different meanings have a large angle between them.\n",
        "\n",
        "Word embeddings is basically a layer which we add to our model. The word embeddings layer converts a word into its vector form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0Ljvxz3FtGf",
        "colab_type": "code",
        "outputId": "878341f1-7160-4414-8890-4883a3b454b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smkh6LT2leaA",
        "colab_type": "code",
        "outputId": "239ada00-e384-4a12-87c9-581355b81817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIkPhLn0lq4i",
        "colab_type": "code",
        "outputId": "cae750b1-cdac-4973-9ce5-16575f0351fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Lets look at one review\n",
        "train_data[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 194,\n",
              " 1153,\n",
              " 194,\n",
              " 8255,\n",
              " 78,\n",
              " 228,\n",
              " 5,\n",
              " 6,\n",
              " 1463,\n",
              " 4369,\n",
              " 5012,\n",
              " 134,\n",
              " 26,\n",
              " 4,\n",
              " 715,\n",
              " 8,\n",
              " 118,\n",
              " 1634,\n",
              " 14,\n",
              " 394,\n",
              " 20,\n",
              " 13,\n",
              " 119,\n",
              " 954,\n",
              " 189,\n",
              " 102,\n",
              " 5,\n",
              " 207,\n",
              " 110,\n",
              " 3103,\n",
              " 21,\n",
              " 14,\n",
              " 69,\n",
              " 188,\n",
              " 8,\n",
              " 30,\n",
              " 23,\n",
              " 7,\n",
              " 4,\n",
              " 249,\n",
              " 126,\n",
              " 93,\n",
              " 4,\n",
              " 114,\n",
              " 9,\n",
              " 2300,\n",
              " 1523,\n",
              " 5,\n",
              " 647,\n",
              " 4,\n",
              " 116,\n",
              " 9,\n",
              " 35,\n",
              " 8163,\n",
              " 4,\n",
              " 229,\n",
              " 9,\n",
              " 340,\n",
              " 1322,\n",
              " 4,\n",
              " 118,\n",
              " 9,\n",
              " 4,\n",
              " 130,\n",
              " 4901,\n",
              " 19,\n",
              " 4,\n",
              " 1002,\n",
              " 5,\n",
              " 89,\n",
              " 29,\n",
              " 952,\n",
              " 46,\n",
              " 37,\n",
              " 4,\n",
              " 455,\n",
              " 9,\n",
              " 45,\n",
              " 43,\n",
              " 38,\n",
              " 1543,\n",
              " 1905,\n",
              " 398,\n",
              " 4,\n",
              " 1649,\n",
              " 26,\n",
              " 6853,\n",
              " 5,\n",
              " 163,\n",
              " 11,\n",
              " 3215,\n",
              " 10156,\n",
              " 4,\n",
              " 1153,\n",
              " 9,\n",
              " 194,\n",
              " 775,\n",
              " 7,\n",
              " 8255,\n",
              " 11596,\n",
              " 349,\n",
              " 2637,\n",
              " 148,\n",
              " 605,\n",
              " 15358,\n",
              " 8003,\n",
              " 15,\n",
              " 123,\n",
              " 125,\n",
              " 68,\n",
              " 23141,\n",
              " 6853,\n",
              " 15,\n",
              " 349,\n",
              " 165,\n",
              " 4362,\n",
              " 98,\n",
              " 5,\n",
              " 4,\n",
              " 228,\n",
              " 9,\n",
              " 43,\n",
              " 36893,\n",
              " 1157,\n",
              " 15,\n",
              " 299,\n",
              " 120,\n",
              " 5,\n",
              " 120,\n",
              " 174,\n",
              " 11,\n",
              " 220,\n",
              " 175,\n",
              " 136,\n",
              " 50,\n",
              " 9,\n",
              " 4373,\n",
              " 228,\n",
              " 8255,\n",
              " 5,\n",
              " 25249,\n",
              " 656,\n",
              " 245,\n",
              " 2350,\n",
              " 5,\n",
              " 4,\n",
              " 9837,\n",
              " 131,\n",
              " 152,\n",
              " 491,\n",
              " 18,\n",
              " 46151,\n",
              " 32,\n",
              " 7464,\n",
              " 1212,\n",
              " 14,\n",
              " 9,\n",
              " 6,\n",
              " 371,\n",
              " 78,\n",
              " 22,\n",
              " 625,\n",
              " 64,\n",
              " 1382,\n",
              " 9,\n",
              " 8,\n",
              " 168,\n",
              " 145,\n",
              " 23,\n",
              " 4,\n",
              " 1690,\n",
              " 15,\n",
              " 16,\n",
              " 4,\n",
              " 1355,\n",
              " 5,\n",
              " 28,\n",
              " 6,\n",
              " 52,\n",
              " 154,\n",
              " 462,\n",
              " 33,\n",
              " 89,\n",
              " 78,\n",
              " 285,\n",
              " 16,\n",
              " 145,\n",
              " 95]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2KjkEcNl0-K",
        "colab_type": "code",
        "outputId": "51a9821b-dc2b-41f5-a1b0-b73e7c65eae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFBeofZsmkzK",
        "colab_type": "text"
      },
      "source": [
        "###More Preprocessing\n",
        "If we have a look at some of our loaded in reviews, we'll notice that they are different lengths. This is an issue. We cannot pass different length data into our neural network. Therefore, we must make each review the same length. To do this we will follow the procedure below:\n",
        "- if the review is greater than 250 words then trim off the extra words\n",
        "- if the review is less than 250 words add the necessary amount of 0's to make it equal to 250. (0's are added to the left side of the sentence i.e before the start of sentence)\n",
        "\n",
        "Luckily for us keras has a function that can do this for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17lM9A-jl5-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkIukVnRm7jN",
        "colab_type": "text"
      },
      "source": [
        "### Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk1dCsU2mSaq",
        "colab_type": "code",
        "outputId": "7c0bf154-18e7-4533-a6ba-651526b0a980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_data[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oztZ8plXnBGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The embedding layer will convert each word to a vector of 32 dimensions. These vectors will be given to LSTM\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hlOhoRnnMQQ",
        "colab_type": "code",
        "outputId": "a411d575-0b32-436d-f7a4-68206ef706e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 32)          2834688   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzKBe1IJnOTg",
        "colab_type": "code",
        "outputId": "2acadab2-18cc-49e7-dfc3-64e0be0306d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 0.4285 - acc: 0.8043 - val_loss: 0.3158 - val_acc: 0.8640\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.2409 - acc: 0.9105 - val_loss: 0.2832 - val_acc: 0.8854\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.1870 - acc: 0.9316 - val_loss: 0.2853 - val_acc: 0.8824\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.1516 - acc: 0.9453 - val_loss: 0.3681 - val_acc: 0.8784\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 0.1342 - acc: 0.9526 - val_loss: 0.2830 - val_acc: 0.8924\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.1167 - acc: 0.9596 - val_loss: 0.3094 - val_acc: 0.8916\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.1027 - acc: 0.9654 - val_loss: 0.3178 - val_acc: 0.8872\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 17s 26ms/step - loss: 0.0896 - acc: 0.9706 - val_loss: 0.3116 - val_acc: 0.8830\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.0810 - acc: 0.9736 - val_loss: 0.4128 - val_acc: 0.8838\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.0728 - acc: 0.9763 - val_loss: 0.3618 - val_acc: 0.8788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJvjsAHDnwE2",
        "colab_type": "code",
        "outputId": "e009f977-5874-477a-f631-e696a83a3430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 5s 7ms/step - loss: 0.4640 - acc: 0.8444\n",
            "[0.46398866176605225, 0.8444399833679199]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KrU2MCio7PC",
        "colab_type": "code",
        "outputId": "0a506da4-5df4-46ab-a0ea-30d806577d6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = \"that movie was just amazing, so amazing\"\n",
        "encoded = encode_text(text)\n",
        "print(encoded)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrKoaOWIq_pU",
        "colab_type": "code",
        "outputId": "a66d7769-dc4f-45a8-b659-9756a363c7c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# while were at it lets make a decode function\n",
        "\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "    PAD = 0\n",
        "    text = \"\"\n",
        "    for num in integers:\n",
        "      if num != PAD:\n",
        "        text += reverse_word_index[num] + \" \"\n",
        "\n",
        "    return text[:-1]\n",
        "  \n",
        "print(decode_integers(encoded))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "that movie was just amazing so amazing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLdGAxMirGuE",
        "colab_type": "code",
        "outputId": "e85f92b7-a5f9-4dfe-bf19-526e310eeb28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# now time to make a prediction\n",
        "\n",
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1 , 250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred) \n",
        "  print(result[0])\n",
        "\n",
        "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8794685]\n",
            "[0.34438893]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYYjyejErTKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}